{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Analysis of Immigration and Temperature data by State\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Main Purpose of this project is analyse the immigration, demographics, temperature and Airport data by State after performing the required all ETL Operation.\n",
    "\n",
    "In the process of building it I have designed and developed to create an star schema comprising of 4 dimension table and an fact table\n",
    "\n",
    "Dimension Table Details\n",
    "1. DIM_IMMIGRATION\n",
    "2. DIM_TEMPERATURE\n",
    "3. DIM_DEMOGRAHICS\n",
    "4. DIM_AIRPORT\n",
    "\n",
    "FACT TABLE Details\n",
    "1. FACT_TABLE\n",
    "\n",
    "ETL Process has the following process\n",
    "\n",
    "##### Extract\n",
    "Data is obtained from all open sources \n",
    "\n",
    "I94 Immigration data is obtained from following source\n",
    "        [ImmigrationData](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "\n",
    "Temperature Data\n",
    "        [Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data/data?select=GlobalLandTemperaturesByState.csv)\n",
    "\n",
    "U.S. City Demographic Data: This data comes from OpenSoft. [Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "\n",
    "Airport Code Table: This is a simple table of airport codes and corresponding cities. [Airport Data](https://datahub.io/core/airport-codes#data)\n",
    "\n",
    "##### Transform\n",
    "\n",
    "Data is transformed after cleaning the data\n",
    "\n",
    "##### Load\n",
    "\n",
    "After performing the cleaning process data is loaded to dimension table and Fact table. These dataframes are written as parquet file\n",
    "These files are partitioned by I94 port. so that people who query these data will have faster results\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create an Spark session to use pyspark as an processing engine\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "Analyse all 4 sources of dat. Using the above 4 sources create anFact table by combining it. The fact table \n",
    "\n",
    "All the stuff has been done using Pyspark operation. In pyspark most of the operations is done by using the Dataframe utilty and SQL operations of pyspark\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. [Here](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "2. World Temperature Data: This dataset came from Kaggle. [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data/data?select=GlobalLandTemperaturesByState.csv)\n",
    "3. U.S. City Demographic Data: This data comes from OpenSoft. [Here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "4. Airport Code Table: This is a simple table of airport codes and corresponding cities. [Here](https://datahub.io/core/airport-codes#data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "demographics_data=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "airport_data=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "temperature_data=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByState.csv\")\n",
    "immigration_data=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|   State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+--------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "+-------------+--------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|    type|             name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|heliport|Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "+-----+--------+-----------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|  4.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  59.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      Z|   null|      U|   null| 1957.0|10032016|  null|  null|   null|1.4938462027E10| null|      WT|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# See the data frame values using show functionality\n",
    "print(demographics_data.show(1))\n",
    "print(airport_data.show(1))\n",
    "print(temperature_data.show(1))\n",
    "print(immigration_data.show(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of demographics_data are as follows\n",
      "StructType(List(StructField(City,StringType,true),StructField(State,StringType,true),StructField(Median Age,StringType,true),StructField(Male Population,StringType,true),StructField(Female Population,StringType,true),StructField(Total Population,StringType,true),StructField(Number of Veterans,StringType,true),StructField(Foreign-born,StringType,true),StructField(Average Household Size,StringType,true),StructField(State Code,StringType,true),StructField(Race,StringType,true),StructField(Count,StringType,true)))\n",
      "\n",
      "Schema of airport_data are as follows\n",
      "StructType(List(StructField(ident,StringType,true),StructField(type,StringType,true),StructField(name,StringType,true),StructField(elevation_ft,StringType,true),StructField(continent,StringType,true),StructField(iso_country,StringType,true),StructField(iso_region,StringType,true),StructField(municipality,StringType,true),StructField(gps_code,StringType,true),StructField(iata_code,StringType,true),StructField(local_code,StringType,true),StructField(coordinates,StringType,true)))\n",
      "\n",
      "Schema of temperature_data are as follows\n",
      "StructType(List(StructField(dt,StringType,true),StructField(AverageTemperature,StringType,true),StructField(AverageTemperatureUncertainty,StringType,true),StructField(State,StringType,true),StructField(Country,StringType,true)))\n",
      "\n",
      "Schema of immigration_data are as follows\n",
      "StructType(List(StructField(cicid,DoubleType,true),StructField(i94yr,DoubleType,true),StructField(i94mon,DoubleType,true),StructField(i94cit,DoubleType,true),StructField(i94res,DoubleType,true),StructField(i94port,StringType,true),StructField(arrdate,DoubleType,true),StructField(i94mode,DoubleType,true),StructField(i94addr,StringType,true),StructField(depdate,DoubleType,true),StructField(i94bir,DoubleType,true),StructField(i94visa,DoubleType,true),StructField(count,DoubleType,true),StructField(validres,DoubleType,true),StructField(delete_days,DoubleType,true),StructField(delete_mexl,DoubleType,true),StructField(delete_dup,DoubleType,true),StructField(delete_visa,DoubleType,true),StructField(delete_recdup,DoubleType,true),StructField(dtadfile,StringType,true),StructField(visapost,StringType,true),StructField(occup,StringType,true),StructField(entdepa,StringType,true),StructField(entdepd,StringType,true),StructField(entdepu,StringType,true),StructField(matflag,StringType,true),StructField(biryear,DoubleType,true),StructField(dtaddto,StringType,true),StructField(gender,StringType,true),StructField(insnum,StringType,true),StructField(airline,StringType,true),StructField(admnum,DoubleType,true),StructField(fltno,StringType,true),StructField(visatype,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "# See the data frame schema\n",
    "print('Schema of demographics_data are as follows')\n",
    "print(demographics_data.schema)\n",
    "print()\n",
    "print('Schema of airport_data are as follows')\n",
    "print(airport_data.schema)\n",
    "print()\n",
    "print('Schema of temperature_data are as follows')\n",
    "print(temperature_data.schema)\n",
    "print()\n",
    "print('Schema of immigration_data are as follows')\n",
    "print(immigration_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns of demographics_data are as follows\n",
      "['City', 'State', 'Median Age', 'Male Population', 'Female Population', 'Total Population', 'Number of Veterans', 'Foreign-born', 'Average Household Size', 'State Code', 'Race', 'Count']\n",
      "\n",
      "columns of airport_data are as follows\n",
      "['ident', 'type', 'name', 'elevation_ft', 'continent', 'iso_country', 'iso_region', 'municipality', 'gps_code', 'iata_code', 'local_code', 'coordinates']\n",
      "\n",
      "columns of temperature_data are as follows\n",
      "['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'State', 'Country']\n",
      "\n",
      "columns of immigration_data are as follows\n",
      "['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count', 'validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa', 'delete_recdup', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline', 'admnum', 'fltno', 'visatype']\n"
     ]
    }
   ],
   "source": [
    "# See the data frame Columns\n",
    "print('columns of demographics_data are as follows')\n",
    "print(demographics_data.columns)\n",
    "print()\n",
    "print('columns of airport_data are as follows')\n",
    "print(airport_data.columns)\n",
    "print()\n",
    "print('columns of temperature_data are as follows')\n",
    "print(temperature_data.columns)\n",
    "print()\n",
    "print('columns of immigration_data are as follows')\n",
    "print(immigration_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Here using the pyspark sql utility I have created each dimension after performing not null check and by taking an distinct distinct values\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. DIM_AIRPORT\n",
    "\n",
    "    1. upper(iso_country) = \"US\" \n",
    "    2. iso_region is not null \n",
    "    3. type = 'heliport'\n",
    "    4. Performed Casting\n",
    "\n",
    "2. DIM_TEMPERATURE\n",
    "\n",
    "    1. trim(upper(Country)) = 'UNITED STATES' \n",
    "    2. AverageTemperature IS NOT NULL \n",
    "    3. AverageTemperatureUncertainty is not null\n",
    "    4. Performed Casting\n",
    "\n",
    "3. DIM_DEMOGRAPHICS\n",
    "    1. Performed Casting\n",
    "\n",
    "4. DIM_IMMIGRATION\n",
    "    1. i94addr is not null\n",
    "    2. i94port not in ('XXX')\n",
    "    3. Performed Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the i94_port.txt and convert it to dictionary\n",
    "i94_df = pd.read_csv('i94port.txt',sep ='=',names = ['3COdeAbrev','values'],header=None)\n",
    "i94_df.head()\n",
    "i94_df['3COdeAbrev'] = i94_df['3COdeAbrev'].str.replace('\\t','').str.strip().str.replace(\"'\",'')\n",
    "i94_df['values'] = i94_df['values'].str.replace('\\t','').str.strip().str.replace(\"'\",'')\n",
    "i94_df['State'] = i94_df['values'].str.split(',').str[0]\n",
    "i94_df['2CodeAbbrev'] = i94_df['values'].str.split(',').str[-1]\n",
    "i94_df.head()\n",
    "i94_spark_df = spark.createDataFrame(i94_df)\n",
    "i94_spark_df.createOrReplaceTempView('StateCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is done to convert STATE to the abbreviation which can be used in later stage to derive the state column\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    'District Of Columbia': 'DC',\n",
    "    'Georgia (State)':'GA'\n",
    "}\n",
    "us_state_abbrev_df = pd.Series(us_state_abbrev).to_frame().reset_index()\n",
    "us_state_abbrev_df.columns = ['State','Abbrev']\n",
    "us_state_abbrev_df.head()\n",
    "us_state_abbrev_df_spark = spark.createDataFrame(us_state_abbrev_df)\n",
    "us_state_abbrev_df_spark.createOrReplaceTempView('us_State_abbrev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+------------+--------------------+\n",
      "|airport_surr_key|                name|state|elevation_ft|         coordinates|\n",
      "+----------------+--------------------+-----+------------+--------------------+\n",
      "|             111|Valdez Hospital H...|   AK|        96.0|-146.345001220703...|\n",
      "|             266|        Otp Heliport|   AK|        38.0|-150.018563, 70.4...|\n",
      "|             316|Campbell BLM Heli...|   AK|       235.0|-149.792007446, 6...|\n",
      "|             354|North Douglas Hel...|   AK|        25.0|-134.496994018554...|\n",
      "|             596| Big Salmon Heliport|   AK|       292.0|-136.013611, 59.4...|\n",
      "|            1119|Era Chulitna Rive...|   AK|       960.0|-150.235992431640...|\n",
      "|            1172|Aviator Hotel Anc...|   AK|       123.0|-149.886482, 61.2...|\n",
      "|            1190|     Pad-66 Heliport|   AK|        60.0|-149.589004516601...|\n",
      "|            1402|Marshall Medical ...|   AL|      1024.0|-86.422975, 34.36...|\n",
      "|            1906|S R P Tolleson Ce...|   AZ|      1140.0|-112.230003356933...|\n",
      "|            1995|Maricopa County S...|   AZ|      1257.0|-111.827003479003...|\n",
      "|            2156|Gilbert Emergency...|   AZ|      1350.0|-111.686996459960...|\n",
      "|            2493|Goldfield Ghost T...|   AZ|      2000.0|-111.491996765136...|\n",
      "|            2877|Sunstate-Glendale...|   AZ|      1135.0|-112.227996826171...|\n",
      "|            2942|Community Regiona...|   CA|       415.0|-119.785004, 36.7...|\n",
      "|            3352|Lugo Substation H...|   CA|      3733.0|-117.370058745, 3...|\n",
      "|            3447|     Camp 8 Heliport|   CA|      1525.0|-118.647003173828...|\n",
      "|            3590|Merle Norman Cosm...|   CA|      1215.0|-118.464996337890...|\n",
      "|            4023|Banner Lassen Med...|   CA|      4419.0|-120.628728, 40.4...|\n",
      "|            4435|The Ritz-Carlton ...|   CA|       149.0|-118.449996948242...|\n",
      "+----------------+--------------------+-----+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# airport_data\n",
    "# CReated an SPark view\n",
    "# using th above created view derived the dimension table\n",
    "# Once the seclect query is excecuted then the spark dimension view is created\n",
    "airport_data.createOrReplaceTempView('airport_data')\n",
    "\n",
    "DIM_AIRPORT = spark.sql(''' \n",
    "select distinct  monotonically_increasing_id() as airport_surr_key, cast(trim(name) as string) as name, \n",
    "trim(sc.2CodeAbbrev) as state,\n",
    "cast(elevation_ft as float) as elevation_ft,\n",
    "cast(coordinates as string) as coordinates\n",
    "from airport_data ad\n",
    "join StateCode sc\n",
    "on trim(upper(cast(substring(ad.iso_region,4,2) as string))) = trim(sc.2CodeAbbrev)\n",
    "where upper(iso_country) = \"US\" and iso_region is not null and type = 'heliport' and elevation_ft is not null \n",
    "and name is not null and coordinates is not null\n",
    "''')\n",
    "DIM_AIRPORT.createOrReplaceTempView('DIM_AIRPORT')\n",
    "DIM_AIRPORT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+-----------------------------+-----+-------------+\n",
      "|temperature_surr_key|        dt|AverageTemperature|AverageTemperatureUncertainty|state|      Country|\n",
      "+--------------------+----------+------------------+-----------------------------+-----+-------------+\n",
      "|         42949673032|1756-07-01|            23.134|                        1.811|   NJ|United States|\n",
      "|         42949673057|1758-09-01|             16.37|                        2.459|   NJ|United States|\n",
      "|         42949673177|1769-07-01|            23.046|                        5.051|   NJ|United States|\n",
      "|         42949673333|1784-12-01|            -0.847|                        2.341|   NJ|United States|\n",
      "|         42949673969|1837-12-01|             0.588|                        4.618|   NJ|United States|\n",
      "|         42949674247|1861-02-01|              1.72|                        1.185|   NJ|United States|\n",
      "|         42949674256|1861-11-01|             5.148|                         1.76|   NJ|United States|\n",
      "|         42949674455|1878-06-01|             18.99|                        1.005|   NJ|United States|\n",
      "|         42949674470|1879-09-01|            16.734|                         0.58|   NJ|United States|\n",
      "|         42949674520|1883-11-01|              6.08|                         0.43|   NJ|United States|\n",
      "|         42949675314|1950-01-01|             4.444|                         0.18|   NJ|United States|\n",
      "|         42949676058|2012-01-01|             1.636|                        0.454|   NJ|United States|\n",
      "|        111669150201|1794-02-01|            -8.098|                        6.441|   WI|United States|\n",
      "|        111669150545|1822-10-01|             5.598|                        1.582|   WI|United States|\n",
      "|        111669151126|1871-03-01|            -0.546|                        1.277|   WI|United States|\n",
      "|        111669151767|1924-08-01|            18.224|                        0.181|   WI|United States|\n",
      "|        111669151888|1934-09-01|            13.986|                        0.182|   WI|United States|\n",
      "|        111669152129|1954-10-01|             8.194|                        0.157|   WI|United States|\n",
      "|        111669152407|1977-12-01|            -8.176|                        0.197|   WI|United States|\n",
      "|        111669152477|1983-10-01|             8.885|                        0.198|   WI|United States|\n",
      "+--------------------+----------+------------------+-----------------------------+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature_data\n",
    "# CReated an SPark view\n",
    "# using th above created view derived the dimension table\n",
    "# Once the seclect query is excecuted then the spark dimension view is created\n",
    "temperature_data.createOrReplaceTempView('temperature_data')\n",
    "\n",
    "DIM_TEMPERATURE = spark.sql(''' \n",
    "select distinct  monotonically_increasing_id() as temperature_surr_key,\n",
    "dt, cast(AverageTemperature as float) as AverageTemperature, cast(AverageTemperatureUncertainty as float) as AverageTemperatureUncertainty\n",
    ", sc.Abbrev as state, Country\n",
    "from temperature_data td\n",
    "join us_State_abbrev sc\n",
    "on trim(upper(cast(td.State as string))) = upper(trim(sc.State))\n",
    "where trim(upper(Country)) = 'UNITED STATES' AND AverageTemperature IS NOT NULL and AverageTemperatureUncertainty is not null\n",
    " ''')\n",
    "DIM_TEMPERATURE.createOrReplaceTempView('DIM_TEMPERATURE')\n",
    "DIM_TEMPERATURE.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------+---------------+-----------------+----------------+----------------+------------+-----+\n",
      "|demographics_surr_key|Median_Age|Male_population|Female_Population|Total_Population|Num_Of_Verterans|Foreign_Born|state|\n",
      "+---------------------+----------+---------------+-----------------+----------------+----------------+------------+-----+\n",
      "|                   31|        38|        91764.0|          97350.0|        189114.0|           16637|       12691|   AL|\n",
      "|                   43|        46|       115712.0|         121132.0|        236844.0|           16798|       27207|   AZ|\n",
      "|                   96|        23|        33224.0|          37093.0|         70317.0|            3667|        4262|   AZ|\n",
      "|                  107|        39|        62875.0|          65567.0|        128442.0|           11109|        9929|   AZ|\n",
      "|                  197|        39|        53817.0|          52757.0|        106574.0|            3782|       56640|   CA|\n",
      "|                  280|        38|        38599.0|          35911.0|         74510.0|            1440|       32752|   CA|\n",
      "|                  430|        32|        54450.0|          49238.0|        103688.0|            7103|       31865|   CA|\n",
      "|                  550|        30|        60989.0|          61247.0|        122236.0|            7484|       21959|   CA|\n",
      "|                 1099|        34|        57856.0|          61624.0|        119480.0|            5551|       31880|   FL|\n",
      "|                 1356|        38|        57198.0|          60481.0|        117679.0|            9063|        3169|   IN|\n",
      "|                 1582|        38|        43793.0|          51166.0|         94959.0|            4185|       19024|   MA|\n",
      "|                 1620|        42|        41985.0|          46824.0|         88809.0|            1814|       21692|   MA|\n",
      "|                 1765|        31|        79039.0|          91772.0|        170811.0|            8146|        1789|   MS|\n",
      "|                 1990|        31|       100135.0|         109673.0|        209808.0|            7288|       17735|   NY|\n",
      "|                 2035|        26|        39897.0|          50691.0|         90588.0|            4283|        4183|   NC|\n",
      "|                 2682|        28|        38614.0|          41198.0|         79812.0|            4322|        4364|   VA|\n",
      "|                 2783|        36|        49151.0|          46037.0|         95188.0|            7135|       18565|   WA|\n",
      "|                  167|        40|        31941.0|          35682.0|         67623.0|            4384|        9735|   CA|\n",
      "|                  219|        38|        38599.0|          35911.0|         74510.0|            1440|       32752|   CA|\n",
      "|                  240|        36|        44397.0|          43039.0|         87436.0|            2790|       17145|   CA|\n",
      "+---------------------+----------+---------------+-----------------+----------------+----------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demographic\n",
    "# CReated an SPark view\n",
    "# using th above created view derived the dimension table\n",
    "# Once the seclect query is excecuted then the spark dimension view is created\n",
    "demographics_data.createOrReplaceTempView('demographics_data')\n",
    "\n",
    "DIM_DEMOGRAPHICS = spark.sql(''' \n",
    "select distinct  monotonically_increasing_id() as demographics_surr_key,\n",
    "cast(`Median Age` as int) as Median_Age, cast(`Male Population` as float) as Male_population, \n",
    "cast(`Female Population` as float) as Female_Population,\n",
    "cast(`Total Population` as float) as Total_Population, \n",
    "cast(`Number of Veterans` as int) as Num_Of_Verterans,\n",
    "cast(`Foreign-born` as int) as Foreign_Born,\n",
    "cast(Abbrev as string ) as state\n",
    "from demographics_data dd\n",
    "join us_State_abbrev sc\n",
    "on trim(upper(cast(dd.State as string))) = upper(trim(sc.State))\n",
    "where `Median Age` is not null and `Total Population` is not null and `Female Population` is not null \n",
    "and cast(`Male Population` as float) is not null\n",
    " ''')\n",
    "DIM_DEMOGRAPHICS.createOrReplaceTempView('DIM_DEMOGRAPHICS')\n",
    "DIM_DEMOGRAPHICS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+--------------+---------+-------+-------+-----+\n",
      "|immigration_surr_key|state| i94yr|i94mon|city_port_name|    cicid|i94visa|i94mode|occup|\n",
      "+--------------------+-----+------+------+--------------+---------+-------+-------+-----+\n",
      "|         85899345966|   MN|2016.0|   6.0|           LOS|3549289.0|    3.0|    1.0|  STU|\n",
      "|        360777252901|   VA|2016.0|   6.0|           BOS|2355046.0|    3.0|    1.0|  STU|\n",
      "|        420906795016|   KY|2016.0|   6.0|           DAL|1309334.0|    3.0|    1.0|  STU|\n",
      "|        558345748999|   CA|2016.0|   6.0|           LOS|2131399.0|    3.0|    1.0|  STU|\n",
      "|        558345749509|   CA|2016.0|   6.0|           LOS|3780229.0|    3.0|    1.0|  STU|\n",
      "|        558345749632|   CA|2016.0|   6.0|           SFR|4004493.0|    3.0|    1.0|  STU|\n",
      "|        558345749663|   CA|2016.0|   6.0|           LOS|4188095.0|    3.0|    1.0|  STU|\n",
      "|        558345749835|   CA|2016.0|   6.0|           SFR|4809535.0|    3.0|    1.0|  GLS|\n",
      "|        558345749841|   CA|2016.0|   6.0|           LOS|4817134.0|    3.0|    1.0|  STU|\n",
      "|        566935683189|   CT|2016.0|   6.0|           BOS|5487038.0|    3.0|    1.0|  STU|\n",
      "|        601295421467|   NC|2016.0|   6.0|           PHI|1302857.0|    2.0|    1.0|  RET|\n",
      "|        601295421494|   NC|2016.0|   6.0|           RDU|3046389.0|    3.0|    1.0|  STU|\n",
      "|        644245094409|   DE|2016.0|   6.0|           LOS|4188099.0|    3.0|    1.0|  STU|\n",
      "|        670014898230|   MO|2016.0|   6.0|           DAL|3904202.0|    3.0|    1.0|  STU|\n",
      "|        687194767504|   IL|2016.0|   6.0|           ATL|2900322.0|    2.0|    1.0|  CMP|\n",
      "|        687194767528|   IL|2016.0|   6.0|           DAL|3631016.0|    3.0|    1.0|  STU|\n",
      "|        781684047972|   WA|2016.0|   6.0|           SEA|3781211.0|    3.0|    1.0|  STU|\n",
      "|        781684048051|   WA|2016.0|   6.0|           NIA|4555738.0|    3.0|    3.0|  STU|\n",
      "|        816043786248|   AL|2016.0|   6.0|           DAL| 722376.0|    3.0|    1.0|  STU|\n",
      "|        987842478279|   PA|2016.0|   6.0|           HOU|3127258.0|    3.0|    1.0|  NRR|\n",
      "+--------------------+-----+------+------+--------------+---------+-------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# immigration\n",
    "# CReated an SPark view\n",
    "# using th above created view derived the dimension table\n",
    "# Once the seclect query is excecuted then the spark dimension view is created\n",
    "immigration_data.createOrReplaceTempView('immigration_data')\n",
    "\n",
    "DIM_IMMIGRATION = spark.sql(''' \n",
    "select distinct  monotonically_increasing_id() as immigration_surr_key,\n",
    "i94addr as state,i94yr,i94mon,i94port as city_port_name,cicid,i94visa,i94mode,occup\n",
    "from immigration_data id \n",
    "join us_State_abbrev sc\n",
    "on trim(upper(cast(id.i94addr as string))) = upper(trim(sc.abbrev))\n",
    "where i94addr is not null and i94port not in ('XXX') and occup is not null and i94port is not null and i94addr is not null\n",
    "and i94yr is not null\n",
    " ''')\n",
    "DIM_IMMIGRATION.createOrReplaceTempView('DIM_IMMIGRATION')\n",
    "DIM_IMMIGRATION.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "##### Dimensions\n",
    "\n",
    "DIM_AIRPORT\n",
    "```\n",
    "    airport_surr_key big int,\n",
    "    name string,\n",
    "    state string,\n",
    "    elevation_ft float,\n",
    "    coordinates string\n",
    "```\n",
    "\n",
    "DIM_TEMPERATURE\n",
    "```\n",
    "    temperature_surr_key bigint,\n",
    "    dtdate,\n",
    "    AverageTemperature float ,\n",
    "    AverageTemperatureUncertainty float, \n",
    "    state varchar, \n",
    "    Country varchar\n",
    "```\n",
    "\n",
    "DIM_DEMOGRAPHICS\n",
    "```\n",
    "    demographics_surr_key big int, \n",
    "    Median_Age float, \n",
    "    Male_population float,\n",
    "    Female_Population float, \n",
    "    Total_Population float, \n",
    "    Num_Of_Verterans float,\n",
    "    Foreign_Born float, \n",
    "    state string\n",
    "```\n",
    "\n",
    "DIM_IMMIGRATION\n",
    "```\n",
    "    immigration_surr_key Big int,\n",
    "    state string,\n",
    "    i94yr float,\n",
    "    i94mon float,\n",
    "    city_port_name string,\n",
    "    cicid float,\n",
    "    i94visa float,\n",
    "    i94mode float,\n",
    "    occup string\n",
    "```\n",
    "##### Fact\n",
    "\n",
    "FINAL_FACT\n",
    "```\n",
    "    state varchar,\n",
    "    i94yr int,\n",
    "    i94mon int,\n",
    "    city_port_name varchar,\n",
    "    Median_Age float, \n",
    "    Male_population float,\n",
    "    Female_Population float, \n",
    "    Total_Population float, \n",
    "    Num_Of_Verterans float,\n",
    "    Foreign_Born float\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.\n",
    "\n",
    "Loaded the following data by partition by state\n",
    "    * DIM_AIRPORT\n",
    "    * DIM_TEMPERATURE\n",
    "    * DIM_DEMOGRAPHICS\n",
    "    * DIM_IMMIGRATION\n",
    "\n",
    "Loaded the following data by partition by state\n",
    "    * FACT_TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the DIM_AIRPORT\n",
      "Load the DIM_TEMPERATURE\n",
      "Load the DIM_DEMOGRAPHICS\n",
      "Load the DIM_IMMIGRATION\n"
     ]
    }
   ],
   "source": [
    "# Write the dimensions partitioned by state\n",
    "print('Load the DIM_AIRPORT')\n",
    "DIM_AIRPORT.write.partitionBy(\"state\").parquet('Final_output' + '/DIM_AIRPORT',mode='overwrite')\n",
    "print('Load the DIM_TEMPERATURE')\n",
    "DIM_TEMPERATURE.write.partitionBy(\"state\").parquet('Final_output' + '/DIM_TEMPERATURE',mode='overwrite')\n",
    "print('Load the DIM_DEMOGRAPHICS')\n",
    "DIM_DEMOGRAPHICS.write.partitionBy(\"state\").parquet('Final_output' + '/DIM_DEMOGRAPHICS',mode='overwrite')\n",
    "print('Load the DIM_IMMIGRATION')\n",
    "DIM_IMMIGRATION.write.partitionBy(\"state\").parquet('Final_output' + '/DIM_IMMIGRATION',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load the fact table after combining the dimension column\n",
    "# hERE Specifically considered 2 of the dimesnion table for joining but this can be enhached to all 4 dimension table\n",
    "FACT_TABLE = spark.sql(\n",
    "'''\n",
    "select  \n",
    "    I.state ,\n",
    "    i94yr ,\n",
    "    i94mon ,\n",
    "    city_port_name ,\n",
    "    Median_Age , \n",
    "    Male_population ,\n",
    "    Female_Population , \n",
    "    Total_Population , \n",
    "    Num_Of_Verterans ,\n",
    "    Foreign_Born   \n",
    "from DIM_IMMIGRATION I\n",
    "join DIM_DEMOGRAPHICS D \n",
    "ON D.STATE = I.STATE\n",
    "where i94yr = '2016' AND OCCUP = 'STU' \n",
    "\n",
    "''')\n",
    "FACT_TABLE.createOrReplaceTempView('FACT_TABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Writing of FACT_TABLE based on Partition on state and month\n",
    "FACT_TABLE.write.partitionBy(\"state\",\"i94mon\").parquet('Final_output' + '/FACT_TABLE',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks\n",
    "\n",
    "Created an Data Quality check if an tables are loaded. By checking the count. if count of records is zero then fail out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "def data_quality_check(df):\n",
    "    '''\n",
    "    :df :param: Spark dataframe Name\n",
    "    '''\n",
    "    \n",
    "    count = df.count()\n",
    "    if count == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(df))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(df, count))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for DataFrame[airport_surr_key: bigint, name: string, state: string, elevation_ft: float, coordinates: string] with 88907 records\n",
      "None\n",
      "Data quality check passed for DataFrame[temperature_surr_key: bigint, dt: string, AverageTemperature: float, AverageTemperatureUncertainty: float, state: string, Country: string] with 141930 records\n",
      "None\n",
      "Data quality check passed for DataFrame[demographics_surr_key: bigint, Median_Age: int, Male_population: float, Female_Population: float, Total_Population: float, Num_Of_Verterans: int, Foreign_Born: int, state: string] with 2875 records\n",
      "None\n",
      "Data quality check passed for DataFrame[immigration_surr_key: bigint, state: string, i94yr: double, i94mon: double, city_port_name: string, cicid: double, i94visa: double, i94mode: double, occup: string] with 10232 records\n",
      "None\n",
      "Data quality check passed for DataFrame[state: string, i94yr: double, i94mon: double, city_port_name: string, Median_Age: int, Male_population: float, Female_Population: float, Total_Population: float, Num_Of_Verterans: int, Foreign_Born: int] with 1352712 records\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_quality_check(DIM_AIRPORT))\n",
    "print(data_quality_check(DIM_TEMPERATURE))\n",
    "print(data_quality_check(DIM_DEMOGRAPHICS))\n",
    "print(data_quality_check(DIM_IMMIGRATION))\n",
    "print(data_quality_check(FACT_TABLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "        Used Python, Jupyter Notebook, Spark, Spark SQL(HIVE)\n",
    "* Propose how often the data should be updated and why.\n",
    "        Data will be updated once an month as this can be used for analysis by year,month\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "        We can use of redshift, EMR (WIth Multiple Nodes) if data got increased\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "        We can schedule it to run every morning 7 am if neccessary using AIrflow\n",
    " * The database needed to be accessed by 100+ people.\n",
    "        We can host it in AWS Cloud. By increasing the number of Nodes it can be accessed by 100 people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
